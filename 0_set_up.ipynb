{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Part 0\n",
    "\n",
    "This notebook explains how to install all the preriquistes and libraries that you will need to run the following tutorials. If you can execute all the following cells, you are good to go.\n",
    "\n",
    "## Environment configuration\n",
    "\n",
    "\n",
    "### Install conda\n",
    "\n",
    "There are two major package managers in Python: pip and conda. For this tutorial we will be using conda which, besides being a package manager is also useful as a version manager. There are two main ways to install conda: Anaconda and Miniconda. Any will be useful for this course, just follow instructions here, according to your operative system:\n",
    "\n",
    "https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html#regular-installation\n",
    "\n",
    "### Create an environment with all the Anaconda libraries\n",
    "\n",
    "    $ conda create --name deeplearning python=3.7 anaconda\n",
    "\n",
    "Don't forget to activate the new env\n",
    "\n",
    "    $ conda activate deeplearning    \n",
    "\n",
    "### Install PyTorch\n",
    "\n",
    "This year we will be using [PyTorch](https://pytorch.org/) as the library to build and train the deep learning models. The library is a little less abstract than other possibilities such as [Keras](https://www.tensorflow.org/guide/keras) but gives a little more control to the user which in turns allows more customisation.\n",
    "\n",
    "In order to install PyTorch we recommend following the [official documentation](https://pytorch.org/get-started/locally/). In your local machine, you will install the version that only has CPU support (i.e. no CUDA version), but in Nabucodonosor you need to install the version with GPU support.\n",
    "\n",
    "#### CPU\n",
    "\n",
    "Install pytorch for CPU:\n",
    "\n",
    "    (deeplearning) $ conda install pytorch torchvision cpuonly -c pytorch\n",
    "    \n",
    "Then just check the version installed is >= 1.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU\n",
    "\n",
    "The GPU PyTorch depends on the CUDA version installed. Nabucodonosor has many installations of CUDA in the `/opt/cuda` directory. You need to add `nvcc` to the `$PATH`. For example, to setup for CUDA 10.2, do the following:\n",
    "\n",
    "    (deeplearning) $ export PATH=/opt/cuda/10.2/bin:$PATH\n",
    "\n",
    "That has to be done every time you enter nabucodonosor, to avoid that add it to your `.bashrc` file:\n",
    "\n",
    "    (deeplearning) $ echo \"export PATH=/opt/cuda/10.2/bin:$PATH\" >> $HOME/.bashrc\n",
    "\n",
    "Then, install the PyTorch library:\n",
    "\n",
    "    (deeplearning) $ conda install pytorch torchvision cudatoolkit=10.2 -c pytorch\n",
    "\n",
    "Check if this is working by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Colab\n",
    "\n",
    "In case you want to install PyTorch on a Google Colab, is possible, but first you need to check what version of `nvcc` is running. For that run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\r\n",
      "Built on Thu_Jan_28_19:32:09_PST_2021\r\n",
      "Cuda compilation tools, release 11.2, V11.2.142\r\n",
      "Build cuda_11.2.r11.2/compiler.29558016_0\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to what the previous cell tells you, you'll need to install the proper drivers, with `pip` instead of conda. Please refer to the [getting started](https://pytorch.org/get-started/locally/) page and check what to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install other libraries\n",
    "\n",
    "We need the `gensim` library to deal with word embeddings, so you need to install it. Plus, the `mlflow` tool to keep track of experiments. Finally, `tqdm` is a handful progress bar to keep track of different processes.\n",
    "\n",
    "    (deeplearning) $ conda install gensim mlflow tqdm -c conda-forge\n",
    "\n",
    "If you have problems importing `gensim` and get this error:\n",
    "\n",
    "    ImportError: cannot import name 'open' from 'smart_open' (C:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\__init__.py)\n",
    "\n",
    "Then try updating `smart_open`:\n",
    "\n",
    "    (deeplearning) $ conda update smart_open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download embeddings and dataset\n",
    "\n",
    "### CIFAR10\n",
    "\n",
    "The dataset we will use (CIFAR10) is part of the `torchvision` package, which makes it fairly easy to download. You can learn more details on it [here](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#loading-and-normalizing-cifar10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8c5ba3798b49dcaada925b87037791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "torchvision.datasets.CIFAR10(root='./data', download=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Embeddings and IMDB reviews Dataset\n",
    "\n",
    "Some examples that we will run for text classification using Convolutional Neural Networks require the Glove Embeddings as well as the IMDB reviews dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  6 65.9M    6 4224k    0     0  12.7M      0  0:00:05 --:--:--  0:00:05 12.6M\r",
      " 58 65.9M   58 38.6M    0     0  29.1M      0  0:00:02  0:00:01  0:00:01 29.1M\r",
      "100 65.9M  100 65.9M    0     0  31.5M      0  0:00:02  0:00:02 --:--:-- 31.5M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      " 29 25.3M   29 7616k    0     0  33.2M      0 --:--:-- --:--:-- --:--:-- 33.0M\r",
      "100 25.3M  100 25.3M    0     0  35.0M      0 --:--:-- --:--:-- --:--:-- 35.0M\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "curl -L https://cs.famaf.unc.edu.ar/\\~ccardellino/resources/diplodatos/glove.6B.50d.txt.gz -o ./data/glove.6B.50d.txt.gz\n",
    "curl -L https://cs.famaf.unc.edu.ar/\\~ccardellino/resources/diplodatos/imdb_reviews.csv.gz -o ./data/imdb_reviews.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeLi Challenge 2019 Dataset\n",
    "\n",
    "For the course project, we will be using a dataset based on the 2019 MeLi Challenge dataset, for automatic classification of products categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meli-challenge-2019/\n",
      "meli-challenge-2019/spanish.test.jsonl.gz\n",
      "meli-challenge-2019/portuguese.validation.jsonl.gz\n",
      "meli-challenge-2019/portuguese.train.jsonl.gz\n",
      "meli-challenge-2019/spanish.train.jsonl.gz\n",
      "meli-challenge-2019/spanish_token_to_index.json.gz\n",
      "meli-challenge-2019/portuguese_token_to_index.json.gz\n",
      "meli-challenge-2019/spanish.validation.jsonl.gz\n",
      "meli-challenge-2019/portuguese.test.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0  945M    0 6848k    0     0  17.7M      0  0:00:53 --:--:--  0:00:53 17.6M\r",
      "  3  945M    3 29.5M    0     0  20.3M      0  0:00:46  0:00:01  0:00:45 20.3M\r",
      "  4  945M    4 46.0M    0     0  18.0M      0  0:00:52  0:00:02  0:00:50 18.0M\r",
      "  6  945M    6 62.3M    0     0  18.3M      0  0:00:51  0:00:03  0:00:48 18.3M\r",
      "  8  945M    8 78.4M    0     0  17.9M      0  0:00:52  0:00:04  0:00:48 17.9M\r",
      "  9  945M    9 91.0M    0     0  16.9M      0  0:00:55  0:00:05  0:00:50 16.8M\r",
      " 11  945M   11  110M    0     0  17.2M      0  0:00:54  0:00:06  0:00:48 16.3M\r",
      " 13  945M   13  126M    0     0  17.1M      0  0:00:55  0:00:07  0:00:48 16.6M\r",
      " 15  945M   15  143M    0     0  17.1M      0  0:00:55  0:00:08  0:00:47 16.3M\r",
      " 16  945M   16  157M    0     0  16.7M      0  0:00:56  0:00:09  0:00:47 15.7M\r",
      " 18  945M   18  175M    0     0  16.8M      0  0:00:55  0:00:10  0:00:45 16.8M\r",
      " 20  945M   20  196M    0     0  17.0M      0  0:00:55  0:00:11  0:00:44 16.8M\r",
      " 22  945M   22  215M    0     0  17.3M      0  0:00:54  0:00:12  0:00:42 17.7M\r",
      " 24  945M   24  231M    0     0  17.3M      0  0:00:54  0:00:13  0:00:41 17.5M\r",
      " 26  945M   26  249M    0     0  17.3M      0  0:00:54  0:00:14  0:00:40 18.4M\r",
      " 28  945M   28  271M    0     0  17.6M      0  0:00:53  0:00:15  0:00:38 19.3M\r",
      " 30  945M   30  292M    0     0  17.8M      0  0:00:52  0:00:16  0:00:36 19.5M\r",
      " 33  945M   33  319M    0     0  18.3M      0  0:00:51  0:00:17  0:00:34 20.8M\r",
      " 36  945M   36  348M    0     0  18.9M      0  0:00:49  0:00:18  0:00:31 23.4M\r",
      " 39  945M   39  374M    0     0  19.3M      0  0:00:48  0:00:19  0:00:29 25.0M\r",
      " 42  945M   42  401M    0     0  19.6M      0  0:00:47  0:00:20  0:00:27 25.8M\r",
      " 45  945M   45  430M    0     0  20.1M      0  0:00:46  0:00:21  0:00:25 27.6M\r",
      " 48  945M   48  456M    0     0  20.3M      0  0:00:46  0:00:22  0:00:24 27.4M\r",
      " 51  945M   51  483M    0     0  20.6M      0  0:00:45  0:00:23  0:00:22 27.0M\r",
      " 53  945M   53  507M    0     0  20.8M      0  0:00:45  0:00:24  0:00:21 26.5M\r",
      " 56  945M   56  534M    0     0  21.0M      0  0:00:44  0:00:25  0:00:19 26.7M\r",
      " 59  945M   59  564M    0     0  21.3M      0  0:00:44  0:00:26  0:00:18 26.7M\r",
      " 62  945M   62  590M    0     0  21.5M      0  0:00:43  0:00:27  0:00:16 26.9M\r",
      " 65  945M   65  619M    0     0  21.8M      0  0:00:43  0:00:28  0:00:15 27.1M\r",
      " 67  945M   67  637M    0     0  21.6M      0  0:00:43  0:00:29  0:00:14 25.9M\r",
      " 69  945M   69  658M    0     0  21.6M      0  0:00:43  0:00:30  0:00:13 24.7M\r",
      " 72  945M   72  680M    0     0  21.6M      0  0:00:43  0:00:31  0:00:12 23.2M\r",
      " 74  945M   74  703M    0     0  21.7M      0  0:00:43  0:00:32  0:00:11 22.5M\r",
      " 76  945M   76  722M    0     0  21.6M      0  0:00:43  0:00:33  0:00:10 20.5M\r",
      " 78  945M   78  742M    0     0  21.5M      0  0:00:43  0:00:34  0:00:09 21.0M\r",
      " 80  945M   80  764M    0     0  21.6M      0  0:00:43  0:00:35  0:00:08 21.1M\r",
      " 83  945M   83  784M    0     0  21.5M      0  0:00:43  0:00:36  0:00:07 20.8M\r",
      " 85  945M   85  810M    0     0  21.6M      0  0:00:43  0:00:37  0:00:06 21.4M\r",
      " 88  945M   88  838M    0     0  21.8M      0  0:00:43  0:00:38  0:00:05 23.1M\r",
      " 90  945M   90  859M    0     0  21.8M      0  0:00:43  0:00:39  0:00:04 23.3M\r",
      " 92  945M   92  878M    0     0  21.7M      0  0:00:43  0:00:40  0:00:03 22.8M\r",
      " 95  945M   95  900M    0     0  21.6M      0  0:00:43  0:00:41  0:00:02 22.5M\r",
      " 97  945M   97  924M    0     0  21.8M      0  0:00:43  0:00:42  0:00:01 22.8M\r",
      "100  945M  100  945M    0     0  21.8M      0  0:00:43  0:00:43 --:--:-- 22.1M\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "curl -L https://cs.famaf.unc.edu.ar/\\~ccardellino/resources/diplodatos/meli-challenge-2019.tar.bz2 -o ./data/meli-challenge-2019.tar.bz2\n",
    "tar jxvf ./data/meli-challenge-2019.tar.bz2 -C ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Nabucodonosor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunneling and ssh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you run a notebook in a remote machine?** You use an ssh connection with a port forwarding. This way, everything that goes to the port on the server machine (like a jupyter notebook) also goes to your localhost.\n",
    "\n",
    "It is likely that everyone will be using the same ports, so we recommend you to select a random number before connecting. The port on the ssh must be the same that you use to start the notebook.\n",
    "\n",
    "```\n",
    "$ ssh -L PORT:localhost:PORT USER@SERVER\n",
    "$ conda activate deeplearning\n",
    "(deeplearning) $ jupyter notebook --port PORT --no-browser\n",
    "```\n",
    "\n",
    "Now you can use the notebook as if it were running on your computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using slurm\n",
    "\n",
    "The Nabucodonosor server uses a queue system called slurm, which grants exclusive access to the CPU resources. You should enqueue everythin you do that takes more than 10 minutes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up\n",
    "\n",
    "1. Download the script https://raw.githubusercontent.com/MIREL-UNC/mirel-scripts/master/run_scripts/submit_job_slurm.sh\n",
    "\n",
    "2. Create a logs folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enqueue things\n",
    "\n",
    "To enqueue a job on slurm, first put your command in a file, for example command.txt\n",
    "```\n",
    "$ sbatch submit_job_slurm.sh commant.txt\n",
    "```\n",
    "\n",
    "The queue will assign your job a number JOBID. All the output of your process will be redirected to logs/JOBID.out and logs/JOBID.err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Controlling things\n",
    "\n",
    "To see the state of the queue run `$ squeue`\n",
    "\n",
    "To cancel a job run `$ scancel JOBID`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoid using GPUs\n",
    "\n",
    "If all the GPUs are being used, you can still force Keras to use the CPU. For simple models this is still a very good option.\n",
    "\n",
    "The easiest way is to run set the environment variable  `CUDA_VISIBLE_DEVICES=\"\"` when running your commands. For example:\n",
    "\n",
    "```\n",
    "(deeplearning) $ CUDA_VISIBLE_DEVICES=\"\" jupyter notebook --no-browser\n",
    "(deeplearning) $ CUDA_VISIBLE_DEVICES=\"\" exercise_1.py --experiment_name mlp_200\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
